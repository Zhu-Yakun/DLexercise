{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自己用tensorflow2改写的原来的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=12.6538, Accuracy=0.1210\n",
      "Epoch 100: Loss=0.8535, Accuracy=0.8760\n",
      "Epoch 200: Loss=0.2675, Accuracy=0.9230\n",
      "Epoch 300: Loss=0.2074, Accuracy=0.9370\n",
      "Epoch 400: Loss=0.2455, Accuracy=0.9480\n",
      "Epoch 500: Loss=0.2204, Accuracy=0.9520\n",
      "Epoch 600: Loss=0.0917, Accuracy=0.9640\n",
      "Epoch 700: Loss=0.1634, Accuracy=0.9660\n",
      "Epoch 800: Loss=0.1870, Accuracy=0.9700\n",
      "Epoch 900: Loss=0.0507, Accuracy=0.9690\n",
      "Epoch 1000: Loss=0.1832, Accuracy=0.9760\n",
      "Epoch 1100: Loss=0.0924, Accuracy=0.9750\n",
      "Epoch 1200: Loss=0.0619, Accuracy=0.9770\n",
      "Epoch 1300: Loss=0.0321, Accuracy=0.9740\n",
      "Epoch 1400: Loss=0.0544, Accuracy=0.9780\n",
      "Epoch 1500: Loss=0.0831, Accuracy=0.9730\n",
      "Epoch 1600: Loss=0.1264, Accuracy=0.9780\n",
      "Epoch 1700: Loss=0.0489, Accuracy=0.9790\n",
      "Epoch 1800: Loss=0.0867, Accuracy=0.9820\n",
      "Epoch 1900: Loss=0.0979, Accuracy=0.9820\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 手动实现数据预处理\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = tf.cast(train_images[..., tf.newaxis]/255.0, tf.float32)  # 添加通道维度并归一化\n",
    "test_images = tf.cast(test_images[..., tf.newaxis]/255.0, tf.float32)\n",
    "train_labels = tf.one_hot(train_labels, 10)  # 手动one-hot编码\n",
    "test_labels = tf.one_hot(test_labels, 10)\n",
    "\n",
    "# 初始化参数\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(tf.random.truncated_normal(shape, stddev=0.1))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape))\n",
    "\n",
    "# 卷积和池化操作\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool2d(x, ksize=2, strides=2, padding='SAME')\n",
    "\n",
    "# 构建网络\n",
    "class ManualCNN(tf.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 卷积层1\n",
    "        self.W_conv1 = weight_variable([7,7,1,32])\n",
    "        self.b_conv1 = bias_variable([32])\n",
    "        \n",
    "        # 卷积层2\n",
    "        self.W_conv2 = weight_variable([5,5,32,64])\n",
    "        self.b_conv2 = bias_variable([64])\n",
    "        \n",
    "        # 全连接层\n",
    "        self.W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "        self.b_fc1 = bias_variable([1024])\n",
    "        self.W_fc2 = weight_variable([1024,10])\n",
    "        self.b_fc2 = bias_variable([10])\n",
    "    \n",
    "    def __call__(self, x, training=False):\n",
    "        # 输入重塑\n",
    "        x = tf.reshape(x, [-1,28,28,1])\n",
    "        \n",
    "        # 卷积层1 + ReLU + 池化\n",
    "        h_conv1 = tf.nn.relu(conv2d(x, self.W_conv1) + self.b_conv1)\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        \n",
    "        # 卷积层2 + ReLU + 池化\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, self.W_conv2) + self.b_conv2)\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "        \n",
    "        # 全连接层\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1,7 * 7 * 64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, self.W_fc1) + self.b_fc1)\n",
    "        \n",
    "        # 训练时应用Dropout\n",
    "        if training:\n",
    "            h_fc1 = tf.nn.dropout(h_fc1, rate=0.3)\n",
    "        \n",
    "        return tf.nn.softmax(tf.matmul(h_fc1, self.W_fc2) + self.b_fc2)\n",
    "\n",
    "# 实例化模型和优化器\n",
    "model = ManualCNN()\n",
    "optimizer = tf.optimizers.Adam(1e-4)\n",
    "\n",
    "# 手动实现训练循环\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        # 手动计算交叉熵损失\n",
    "        loss = -tf.reduce_mean(tf.reduce_sum(labels * tf.math.log(predictions), axis=1))\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# 计算准确率\n",
    "def compute_accuracy(images, labels):\n",
    "    predictions = model(images)\n",
    "    correct = tf.equal(tf.argmax(predictions,1), tf.argmax(labels,1))\n",
    "    return tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(2000):\n",
    "    # 生成batch\n",
    "    idx = np.random.choice(len(train_images), 100)\n",
    "    batch_images = tf.gather(train_images, idx)\n",
    "    batch_labels = tf.gather(train_labels, idx)\n",
    "    \n",
    "    loss = train_step(batch_images, batch_labels)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        acc = compute_accuracy(test_images[:1000], test_labels[:1000])\n",
    "        print(f\"Epoch {epoch}: Loss={loss:.4f}, Accuracy={acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "deeplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
